{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Natural Language Toolkit is beneficiary when there is a requirement of doing statistical natural language processing or significant amounts of machine learning on text.\n",
    "\n",
    "NLTK Corpora - A corpus is a huge collection of written text. \n",
    "         A compilation of corpuses is called a corpora...\n",
    "         it is a body of texts used for analysis and development of NLP tools..\n",
    "    To Download - import nltk\n",
    "                  nltk.download(corpus_name)\n",
    "                  \n",
    "                  \n",
    "Text cleaning techniques - \n",
    "       Basics - removing Punctuation\n",
    "                converting text into lower or upper case\n",
    "                removing stopwords\n",
    "                removing no sensible words e.g. \\n\n",
    "                tokenizing\n",
    "                lemmantization and stemming - conveting to root form\n",
    "                handling n-grams\n",
    "      Advanced - Normalization\n",
    "                 correction of typos (mistakes)\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The process of turning string or a text paragraph into smaller chunks or tokens such as words, sentences, phrases, symbols, keywords..\n",
    "Tokenization Methods: sent_tokenize\n",
    "                      word_tokenize\n",
    "                      RegexpTokenizer\n",
    "                      BlanklineTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize            # split the document into sentences\n",
    "from nltk.tokenize import word_tokenize          # split the document or sentence into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Hello Everyone. Welcome here. We are here.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = sent_tokenize(text=dataset, language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Everyone.\n",
      "Welcome here.\n",
      "We are here.\n"
     ]
    }
   ],
   "source": [
    "for i in d:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = word_tokenize(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'Everyone', '.', 'Welcome', 'here', '.', 'We', 'are', 'here', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Everyone\n",
      "Welcome\n",
      "here\n",
      "We\n",
      "are\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "for j in w:\n",
    "    if j!='.':\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It's a process for text normalization which involves reducing words to their base/root words from their derived words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['love','loving','lover','loved','loves','lovingly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PorterStemmer>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = PorterStemmer()\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "love\n",
      "lover\n",
      "love\n",
      "love\n",
      "lovingli\n"
     ]
    }
   ],
   "source": [
    "for i in data:\n",
    "    print(st.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Hello Everyone. Welcome here. We are here.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "everyon\n",
      ".\n",
      "welcom\n",
      "here\n",
      ".\n",
      "We\n",
      "are\n",
      "here\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(st.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It is a process which involves reducing words to their root/base words using vocabulary and morphological analysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'church'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize('churches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize('dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize('better', pos='a')       # pos shows Part Of Speech.. here 'a' showing better is an adjective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP, useless words are called stopwords.\n",
    "NLTK has list of stopwords stored is 16 different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Hello Everyone. Welcome here. We are here. Weather is awesome'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create set of stopwords in english...\n",
    "sw = set(stopwords.words('english'))\n",
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tok = word_tokenize(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Everyone',\n",
       " '.',\n",
       " 'Welcome',\n",
       " 'here',\n",
       " '.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'here',\n",
       " '.',\n",
       " 'Weather',\n",
       " 'is',\n",
       " 'awesome']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords from dataset..\n",
    "\n",
    "filtered = []\n",
    "for w in word_tok:\n",
    "    if w not in sw:\n",
    "        filtered.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'Everyone', '.', 'Welcome', '.', 'We', '.', 'Weather', 'awesome']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part of Speech Tagging - it's a process of assigning one of the parts of speech to a given word... e.g. word:Paper, Tag:Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Tajmahal is one of the world\"s most celebrated structures. Indian History.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_tok = word_tokenize(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = pos_tag(wr_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>POS_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tajmahal</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>world</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>''</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>s</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>most</td>\n",
       "      <td>JJS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>celebrated</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>structures</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Indian</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>History</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Words POS_tag\n",
       "0     Tajmahal     NNP\n",
       "1           is     VBZ\n",
       "2          one      CD\n",
       "3           of      IN\n",
       "4          the      DT\n",
       "5        world      NN\n",
       "6           ''      ''\n",
       "7            s     VBZ\n",
       "8         most     JJS\n",
       "9   celebrated      JJ\n",
       "10  structures     NNS\n",
       "11           .       .\n",
       "12      Indian      JJ\n",
       "13     History     NNP\n",
       "14           .       ."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(tag, columns=['Words','POS_tag'])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "# set of tags....\n",
    "\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking... Grouping the Information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Taj mahal is one of the world\"s most celebrated structures. Indian History.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtok = word_tokenize(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pstag = pos_tag(wtok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_chunk = \"\"\"\n",
    "chunk:\n",
    "{<NNPS>}\n",
    "{<NNP>+}\n",
    "{<NN>+}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = RegexpParser(sequence_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_result = chunk.parse(pstag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (chunk Taj/NNP)\n",
      "  (chunk mahal/NN)\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (chunk world/NN)\n",
      "  ''/''\n",
      "  s/VBZ\n",
      "  most/JJS\n",
      "  celebrated/JJ\n",
      "  structures/NNS\n",
      "  ./.\n",
      "  Indian/JJ\n",
      "  (chunk History/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(chunk_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition... e.g. Tesla:Organization, Elon Musk:Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Abraham Lincoln was an American statesman and lawyer, who served as the 16th President od USA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = pos_tag(word_tokenize(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Abraham', 'NNP'),\n",
       " ('Lincoln', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('an', 'DT'),\n",
       " ('American', 'JJ'),\n",
       " ('statesman', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('lawyer', 'NN'),\n",
       " (',', ','),\n",
       " ('who', 'WP'),\n",
       " ('served', 'VBD'),\n",
       " ('as', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('16th', 'CD'),\n",
       " ('President', 'NNP'),\n",
       " ('od', 'MD'),\n",
       " ('USA', 'NNP')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = ne_chunk(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Abraham/NNP)\n",
      "  (PERSON Lincoln/NNP)\n",
      "  was/VBD\n",
      "  an/DT\n",
      "  (GPE American/JJ)\n",
      "  statesman/NN\n",
      "  and/CC\n",
      "  lawyer/NN\n",
      "  ,/,\n",
      "  who/WP\n",
      "  served/VBD\n",
      "  as/IN\n",
      "  the/DT\n",
      "  16th/CD\n",
      "  President/NNP\n",
      "  od/MD\n",
      "  (ORGANIZATION USA/NNP))\n"
     ]
    }
   ],
   "source": [
    "print(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\SANDIP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')           # library brown has a collection of words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matches\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(text.similar('sandip'))           # to find the words similar to 'woman'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in of to on for at as that with is or but was from when by it if the\n",
      "all\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(text.similar(\"and\"))         # to find the words similar to 'and'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a summary of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_stop_words = nltk.corpus.stopwords.words('english')\n",
    "len(eng_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('E:\\Sandip\\Python was conceived in the late 1980s.txt', 'r')\n",
    "my_file = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1950"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python was conceived in the late     s     by Guido van Rossum at Centrum Wiskunde   Informatica  CWI  in the Netherlands as a successor to ABC programming language  which was inspired by SETL      capable of exception handling and interfacing with the Amoeba operating system      Its implementation began in December           Van Rossum shouldered sole responsibility for the project  as the lead developer  until    July       when he announced his  permanent vacation  from his responsibilities as Python s Benevolent Dictator For Life  a title the Python community bestowed upon him to reflect his long term commitment as the project s chief decision maker      In January       active Python core developers elected a   member  Steering Council  to lead the project      As of       the current members of this council are Barry Warsaw  Brett Cannon  Carol Willing  Thomas Wouters  and Pablo Galindo Salgado      Python     was released on    October       with many major new features  including a cycle detecting garbage collector and support for Unicode      Python     was released on   December       It was a major revision of the language that is not completely backward compatible      Many of its major features were backported to Python     x     and     x version series  Releases of Python   include the  to  utility  which automates  at least partially  the translation of Python   code to Python        Python     s end of life date was initially set at      then postponed to      out of concern that a large body of existing code could not easily be forward ported to Python            No more security patches or other improvements will be released for it          With Python   s end of life  only Python     x     and later are supported  Python       and       were expedited     as all versions of Python  including          had security issues  leading to possible remote code execution     and web cache poisoning       '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = re.sub(r'[^a-zA-Z]',' ',my_file)           # remove unwanted characters...\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docs.lower()     # convert into lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python was conceived in the late     s     by guido van rossum at centrum wiskunde   informatica  cwi  in the netherlands as a successor to abc programming language  which was inspired by setl      capable of exception handling and interfacing with the amoeba operating system      its implementation began in december           van rossum shouldered sole responsibility for the project  as the lead developer  until    july       when he announced his  permanent vacation  from his responsibilities as python s benevolent dictator for life  a title the python community bestowed upon him to reflect his long term commitment as the project s chief decision maker      in january       active python core developers elected a   member  steering council  to lead the project      as of       the current members of this council are barry warsaw  brett cannon  carol willing  thomas wouters  and pablo galindo salgado      python     was released on    october       with many major new features  including a cycle detecting garbage collector and support for unicode      python     was released on   december       it was a major revision of the language that is not completely backward compatible      many of its major features were backported to python     x     and     x version series  releases of python   include the  to  utility  which automates  at least partially  the translation of python   code to python        python     s end of life date was initially set at      then postponed to      out of concern that a large body of existing code could not easily be forward ported to python            no more security patches or other improvements will be released for it          with python   s end of life  only python     x     and later are supported  python       and       were expedited     as all versions of python  including          had security issues  leading to possible remote code execution     and web cache poisoning'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = docs.strip()     # removing leading and trailing spaces\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python',\n",
       " 'conceived',\n",
       " 'late',\n",
       " 'guido',\n",
       " 'van',\n",
       " 'rossum',\n",
       " 'centrum',\n",
       " 'wiskunde',\n",
       " 'informatica',\n",
       " 'cwi',\n",
       " 'netherlands',\n",
       " 'successor',\n",
       " 'abc',\n",
       " 'programming',\n",
       " 'language',\n",
       " 'inspired',\n",
       " 'setl',\n",
       " 'capable',\n",
       " 'exception',\n",
       " 'handling',\n",
       " 'interfacing',\n",
       " 'amoeba',\n",
       " 'operating',\n",
       " 'system',\n",
       " 'implementation',\n",
       " 'began',\n",
       " 'december',\n",
       " 'van',\n",
       " 'rossum',\n",
       " 'shouldered',\n",
       " 'sole',\n",
       " 'responsibility',\n",
       " 'project',\n",
       " 'lead',\n",
       " 'developer',\n",
       " 'july',\n",
       " 'announced',\n",
       " 'permanent',\n",
       " 'vacation',\n",
       " 'responsibilities',\n",
       " 'python',\n",
       " 'benevolent',\n",
       " 'dictator',\n",
       " 'life',\n",
       " 'title',\n",
       " 'python',\n",
       " 'community',\n",
       " 'bestowed',\n",
       " 'upon',\n",
       " 'reflect',\n",
       " 'long',\n",
       " 'term',\n",
       " 'commitment',\n",
       " 'project',\n",
       " 'chief',\n",
       " 'decision',\n",
       " 'maker',\n",
       " 'january',\n",
       " 'active',\n",
       " 'python',\n",
       " 'core',\n",
       " 'developers',\n",
       " 'elected',\n",
       " 'member',\n",
       " 'steering',\n",
       " 'council',\n",
       " 'lead',\n",
       " 'project',\n",
       " 'current',\n",
       " 'members',\n",
       " 'council',\n",
       " 'barry',\n",
       " 'warsaw',\n",
       " 'brett',\n",
       " 'cannon',\n",
       " 'carol',\n",
       " 'willing',\n",
       " 'thomas',\n",
       " 'wouters',\n",
       " 'pablo',\n",
       " 'galindo',\n",
       " 'salgado',\n",
       " 'python',\n",
       " 'released',\n",
       " 'october',\n",
       " 'many',\n",
       " 'major',\n",
       " 'new',\n",
       " 'features',\n",
       " 'including',\n",
       " 'cycle',\n",
       " 'detecting',\n",
       " 'garbage',\n",
       " 'collector',\n",
       " 'support',\n",
       " 'unicode',\n",
       " 'python',\n",
       " 'released',\n",
       " 'december',\n",
       " 'major',\n",
       " 'revision',\n",
       " 'language',\n",
       " 'completely',\n",
       " 'backward',\n",
       " 'compatible',\n",
       " 'many',\n",
       " 'major',\n",
       " 'features',\n",
       " 'backported',\n",
       " 'python',\n",
       " 'x',\n",
       " 'x',\n",
       " 'version',\n",
       " 'series',\n",
       " 'releases',\n",
       " 'python',\n",
       " 'include',\n",
       " 'utility',\n",
       " 'automates',\n",
       " 'least',\n",
       " 'partially',\n",
       " 'translation',\n",
       " 'python',\n",
       " 'code',\n",
       " 'python',\n",
       " 'python',\n",
       " 'end',\n",
       " 'life',\n",
       " 'date',\n",
       " 'initially',\n",
       " 'set',\n",
       " 'postponed',\n",
       " 'concern',\n",
       " 'large',\n",
       " 'body',\n",
       " 'existing',\n",
       " 'code',\n",
       " 'could',\n",
       " 'easily',\n",
       " 'forward',\n",
       " 'ported',\n",
       " 'python',\n",
       " 'security',\n",
       " 'patches',\n",
       " 'improvements',\n",
       " 'released',\n",
       " 'python',\n",
       " 'end',\n",
       " 'life',\n",
       " 'python',\n",
       " 'x',\n",
       " 'later',\n",
       " 'supported',\n",
       " 'python',\n",
       " 'expedited',\n",
       " 'versions',\n",
       " 'python',\n",
       " 'including',\n",
       " 'security',\n",
       " 'issues',\n",
       " 'leading',\n",
       " 'possible',\n",
       " 'remote',\n",
       " 'code',\n",
       " 'execution',\n",
       " 'web',\n",
       " 'cache',\n",
       " 'poisoning']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize = nltk.word_tokenize(docs)         # tokenize the docs into words\n",
    "tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = [token for token in tokenize if token not in eng_stop_words]        # remove the stop words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ' '.join(filtered)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python conceived late guido van rossum centrum wiskunde informatica cwi netherlands successor abc programming language inspired setl capable exception handling interfacing amoeba operating system implementation began december van rossum shouldered sole responsibility project lead developer july announced permanent vacation responsibilities python benevolent dictator life title python community bestowed upon reflect long term commitment project chief decision maker january active python core developers elected member steering council lead project current members council barry warsaw brett cannon carol willing thomas wouters pablo galindo salgado python released october many major new features including cycle detecting garbage collector support unicode python released december major revision language completely backward compatible many major features backported python x x version series releases python include utility automates least partially translation python code python python end life date initially set postponed concern large body existing code could easily forward ported python security patches improvements released python end life python x later supported python expedited versions python including security issues leading possible remote code execution web cache poisoning'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1297"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced cleaning Technique - Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from normalise import normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'On the 30th Jan 2020, corona virus hit india with 1st case in Kerala anywhere G.O.I started acting and allocated fund of 17287 Crores I.N.R'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'normalise_token: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'On the thirtieth of Jan twenty twenty , corona virus hit india with first case in Kerala anywhere Government Of India started acting and allocated fund of seventeen thousand, two hundred and eighty seven Crores Indian Rupees'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "abbr = {'G.O.I': 'Government Of India',\n",
    "       'I.N.R':'Indian Rupees'\n",
    "       }\n",
    "normalise_token = normalise(word_tokenize(txt), user_abbrevs = abbr, verbose = False)\n",
    "display('normalise_token: ',{' '.join(normalise_token)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction in Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF  -->  Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer performs the task of tokenizing and counting, while TfidfTransformer normalizes the data. \n",
    "TfidfVectorizer, on the other hand, performs all three operations, thereby streamlining the process of natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer           # CountVectorizer implements both tokenization and occurrence counting in a single class..\n",
    "\n",
    "text = ['hello, my name is Sandip and I am a data scientist']\n",
    "text1 = ['You are watching name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize = CountVectorizer()\n",
    "\n",
    "vectorize.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 3, 'my': 5, 'name': 6, 'is': 4, 'sandip': 7, 'and': 1, 'am': 0, 'data': 2, 'scientist': 8}\n"
     ]
    }
   ],
   "source": [
    "# summarize\n",
    "print(vectorize.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newvector:    (0, 6)\t1\n",
      "newvector_array:  [[0 0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# encode document\n",
    "newvector = vectorize.transform(text1)\n",
    "\n",
    "print('Newvector: ',newvector)\n",
    "print('newvector_array: ',newvector.toarray())             # in the 'text' name is at 6th position, hence 'text1' has similar word, it's showing 1 at 6th position..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of TF-IDF is to highlight the words which are frequent in document but not across the documents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = ['Sandip is a Data Scientist in India.','Data Science is a promising field']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sandip': 6, 'is': 4, 'data': 0, 'scientist': 8, 'in': 2, 'india': 3, 'science': 7, 'promising': 5, 'field': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         1.40546511 1.40546511 1.40546511 1.         1.40546511\n",
      " 1.40546511 1.40546511 1.40546511]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sandip is a Data Scientist in India.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_input = text[0]\n",
    "text_as_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the document\n",
    "vector = vectorize.transform([text_as_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding --- means numerical representation of text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Types of word embedding:\n",
    "Frequency based embedding - 1. count vector -how many time the perticular word occurs\n",
    "                            2. TF-IDF vector  -how many times some perticular word occurs across all the documents\n",
    "                            3. Co-Occurance vector  - how many times the combination of two, three or more words occur together\n",
    "        \n",
    "Prediction based embedding:\n",
    "                            1. CBOW  \n",
    "                            2. Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Using creating Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Keras embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
